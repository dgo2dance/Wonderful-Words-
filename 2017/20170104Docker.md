
### 什么是Docker

Docker是一个虚拟化容器，其目标是实现轻量级的操作系统虚拟化解决方案。Docker的基础是Linux等技术。
本文操作的是在Ubuntu系统上。
 

### docker安装

	sudo apt-get install docker.io
	
	sudo docker service start
	
	sudo docker version 

 

### 添加用户组

sudo 为临时以管理员权限执行，每次执行都需要加sudo，所以新添加用户组，进行授权。

	sudo group add docker
	
	sudo gpasswd  -a will docker
	
	sudo service docker restart
	
	docker version #检查是否生效,若未生效，重启系统
	
	sudo reboot

 
### 拉取镜像

从hub.docker.com拉取我们需要的ubuntu镜像

	docker pull ubuntu
	
	docker images \#列出所有本地镜像  
  
	docker run -ti ubuntu \#启动容器

### JAVA安装 ###

	apt-get update  
  
	apt-get install python-software-properties  
	  
	apt-get install software-properties-common    
	  
	add-apt-repository ppa:webupd8team/java  \#注册资源库地址  
	  
	apt-get update  
	  
	apt-get install oracle-java8-installer  \#安装JAVA8   
	  
	java -version  \#查看jav版本  
	  
	apt-get remove oracle-java8-installer  \#卸载安装

 
### Docker Commit ###
 
	exit  
	  
	docker commit -m "java install" 122a2cecdd14 ubuntu:java  
	
	#122a2cecdd14为Docker Container容器的ID

	docker run -ti ubuntu:java

### Hadoop安装 ###
	
	#启动前面提交的容器
	docker run -ti ubuntu:java
	#安装wget
	sudo apt-get install -y wget
	#下载并解压安装文件
	root@9ef06706f48d:cd ~
	root@9ef06706f48d:~# mkdir soft
	root@9ef06706f48d:~# cd soft/
	root@9ef06706f48d:~/soft# mkdir apache
	root@9ef06706f48d:~/soft# cd apache/
	root@9ef06706f48d:~/soft/apache# mkdir hadoop
	root@9ef06706f48d:~/soft/apache# cd hadoop/
	root@9ef06706f48d:~/soft/apache/hadoop# wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
	root@8ef06706f88d:~/soft/apache/hadoop# tar xvzf hadoop-2.6.0.tar.gz
	#配置环境变量
	修改~/.bashrc文件，在末尾加入如下几行
	export JAVA_HOME=/usr/lib/jvm/java-7-oracle
	export HADOOP_HOME=/root/soft/apache/hadoop/hadoop-2.6.0
	export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop
	export PATH=$PATH:$HADOOP_HOME/bin
	export PATH=$PATH:$HADOOP_HOME/sbin
	#默认的docker ubuntu可能没安装vi命令，可自行安装vim

### 配置Hadoop ###
	
主要为3个配置文件，core-site.xml、hdfs-site.xml、mapred-site.xml

	root@9ef06706f48d:~# cd $HADOOP_HOME/
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0# mkdir tmp
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0# cd tmp/
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0/tmp# pwd
	/root/soft/apache/hadoop/hadoop-2.6.0/tmp
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0/tmp# cd ../
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0# mkdir namenode
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0# cd namenode/
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0/namenode# pwd
	/root/soft/apache/hadoop/hadoop-2.6.0/namenode
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0/namenode# cd ../
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0# mkdir datanode
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0# cd datanode/
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0/datanode# pwd
	/root/soft/apache/hadoop/hadoop-2.6.0/datanode
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0/datanode# cd $HADOOP_CONFIG_HOME/
	root@9ef06706f48d:~/soft/apache/hadoop/hadoop-2.6.0/etc/hadoop# cp mapred-site.xml.template mapred-site.xml
	
创建了三个目录，tmp  namenode   datanode
其中namenode：作为NameNode的存放目录  datanode：作为DataNode的存放目录

1）core-site.xml配置
	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<!--
 		 Licensed under the Apache License, Version 2.0 (the "License");
  	you may not use this file except in compliance with the License.
  	You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  	Unless required by applicable law or agreed to in writing, software
  	distributed under the License is distributed on an "AS IS" BASIS,
  	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 	 See the License for the specific language governing permissions and
 	 limitations under the License. See accompanying LICENSE file.
	-->

	<!-- Put site-specific property overrides in this file. -->

	<configuration>
    <property>
            <name>hadoop.tmp.dir</name>
            <value>/root/soft/apache/hadoop/hadoop-2.6.0/tmp</value>
            <description>A base for other temporary directories.</description>
    </property>

    <property>
            <name>fs.default.name</name>
            <value>hdfs://master:9000</value>
            <final>true</final>
            <description>The name of the default file system.  A URI whose
            scheme and authority determine the FileSystem implementation.  The
            uri's scheme determines the config property (fs.SCHEME.impl) naming
            the FileSystem implementation class.  The uri's authority is used to
            determine the host, port, etc. for a filesystem.</description>
    </property>
	</configuration>	

hadoop.tmp.dir配置项值即为此前命令中创建的临时目录路径，fs.default.name配置为hdfs://master:9000，指向的是一个Master节点的主机（后续我们做集群配置的时候，自然会配置这个节点，先写在这里）
2）hdfs-site.xml配置	
		
	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<!--
	  Licensed under the Apache License, Version 2.0 (the "License");
	  you may not use this file except in compliance with the License.
	  You may obtain a copy of the License at
	
	    http://www.apache.org/licenses/LICENSE-2.0
	
	  Unless required by applicable law or agreed to in writing, software
	  distributed under the License is distributed on an "AS IS" BASIS,
	  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
	  See the License for the specific language governing permissions and
	  limitations under the License. See accompanying LICENSE file.
	-->
	
	<!-- Put site-specific property overrides in this file. -->
	
	<configuration>
	    <property>
	        <name>dfs.replication</name>
	        <value>2</value>
	        <final>true</final>
	        <description>Default block replication.
	        The actual number of replications can be specified when the file is created.
	        The default is used if replication is not specified in create time.
	        </description>
	    </property>
	
	    <property>
	        <name>dfs.namenode.name.dir</name>
	        <value>/root/soft/apache/hadoop/hadoop-2.6.0/namenode</value>
	        <final>true</final>
	    </property>
	
	    <property>
	        <name>dfs.datanode.data.dir</name>
	        <value>/root/soft/apache/hadoop/hadoop-2.6.0/datanode</value>
	        <final>true</final>
	    </property>
	</configuration>

我们后续搭建集群环境时，将配置一个Master节点和两个Slave节点。所以dfs.replication配置为2。
dfs.namenode.name.dir和dfs.datanode.data.dir分别配置为之前创建的NameNode和DataNode的目录路径

3）mapred-site.xml配置
之前使用了命令cp mapred-site.xml.template mapred-site.xml，创建了一个mapred-site.xml文件。编辑这个文件内容为：
	<?xml version="1.0"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<!--
	  Licensed under the Apache License, Version 2.0 (the "License");
	  you may not use this file except in compliance with the License.
	  You may obtain a copy of the License at
	
	    http://www.apache.org/licenses/LICENSE-2.0
	
	  Unless required by applicable law or agreed to in writing, software
	  distributed under the License is distributed on an "AS IS" BASIS,
	  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
	  See the License for the specific language governing permissions and
	  limitations under the License. See accompanying LICENSE file.
	-->
	
	<!-- Put site-specific property overrides in this file. -->
	
	<configuration>
	    <property>
	        <name>mapred.job.tracker</name>
	        <value>master:9001</value>
	        <description>The host and port that the MapReduce job tracker runs
	        at.  If "local", then jobs are run in-process as a single map
	        and reduce task.
	        </description>
	    </property>
	</configuration>

这里只有一个配置项mapred.job.tracker，我们指向master节点机器。

4）修改JAVA_HOME环境变量
	export JAVA_HOME=/usr/lib/jvm/java-7-oracle

5）格式化 namenode
	hadoop namenode -format

### 安装SSH ###

	sudo apt-get install ssh
	#然后配置ssh为无密码访问，当然也可以设置访问秘钥

### 保存镜像 ###
	
	exit
	docker commit -m "hadoop install" 9ef06706f48d ubuntu:hadoop

### Hadoop集群搭建 ###

以一个Master节点，两个slave 节点为例

1)启动Docker容器
docker run -ti ubuntu:hadoop

有两个说明的

- Docker容器中的ip地址是启动之后自动分配的，且不能手动更改
- hostname、hosts配置在容器内修改了，只能在本次容器生命周期内有效。如果容器退出了，重新启动，这两个配置将被还原。且这两个配置无法通过commit命令写入镜像

我们搭建集群环境的时候，需要指定节点的hostname，以及配置hosts。hostname可以使用Docker run命令的h参数直接指定。hosts解析有点麻烦，有些单独配置。

2）启动master容器
	docker run -ti -h master ubuntu:hadoop
	docker run -ti -h slave1 ubuntu:hadoop
	docker run -ti -h slave2 ubuntu:hadoop

3）配置host
先用ifconfig命令获取各节点ip,如果命令没安装，使用apt-get install net-tools安装

	master:172.10.0.2
	slave1:172.10.0.3
	slave1:172.10.0.4
依次修改各个HOST

4) 启动hadoop 
在master执行命令start-all.sh，启动hadoop

如显示信息无报错，则执行成功

可以用jps命令。查看状态。

也可以用hdfs dfsadmin -report查看DataNode是否正常。

后续，则可以开始执行WordCount实例。

后面会继续逐步说明WordCount执行及DNS解析的搭建。